"""
–ú–æ–¥—É–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –¥–ª—è ShadowFlow
–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞—Ç–∞–∫, —Ä–∞–Ω–Ω–µ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –∏ —Ä–∏—Å–∫-—Å–∫–æ—Ä–∏–Ω–≥
"""

import json
import os
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
from collections import defaultdict, deque
import warnings
warnings.filterwarnings('ignore')

# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
try:
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import classification_report, accuracy_score
    from sklearn.linear_model import LogisticRegression
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

class PredictiveAnalytics:
    def __init__(self, cache_file: str = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
        
        Args:
            cache_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫—ç—à–∞ —Å –¥–∞–Ω–Ω—ã–º–∏
        """
        if cache_file is None:
            if os.path.exists("/app"):
                # Docker –æ–∫—Ä—É–∂–µ–Ω–∏–µ
                self.cache_file = "/app/data/cache.json"
            else:
                # –õ–æ–∫–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
                self.cache_file = "/Users/Kos/shadowflow/data/cache.json"
        else:
            self.cache_file = cache_file
            
        self.models = {}
        self.scalers = {}
        self.feature_history = deque(maxlen=1000)  # –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.alert_history = deque(maxlen=100)     # –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
        self.models_trained = False                # –§–ª–∞–≥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        self.cache_dir = os.path.join(os.path.dirname(self.cache_file), 'models')
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
        self.risk_thresholds = {
            'low': 0.3,
            'medium': 0.6,
            'high': 0.8,
            'critical': 0.9
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self._initialize_models()
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        self._load_saved_models()
    
    def _initialize_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ML –º–æ–¥–µ–ª–µ–π"""
        print("üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏...")
        
        if SKLEARN_AVAILABLE:
            self.models['random_forest'] = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )
            self.models['gradient_boosting'] = GradientBoostingClassifier(
                n_estimators=100,
                max_depth=6,
                random_state=42
            )
            self.models['logistic_regression'] = LogisticRegression(
                random_state=42,
                max_iter=1000
            )
            self.scalers['standard'] = StandardScaler()
        
        if XGBOOST_AVAILABLE:
            self.models['xgboost'] = xgb.XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            )
        
        if LIGHTGBM_AVAILABLE:
            self.models['lightgbm'] = lgb.LGBMClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            )
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(self.models)}")
    
    def _save_models(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
        try:
            import pickle
            
            for name, model in self.models.items():
                model_path = os.path.join(self.cache_dir, f"{name}_model.pkl")
                with open(model_path, 'wb') as f:
                    pickle.dump(model, f)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–µ–π–ª–µ—Ä—ã
            for name, scaler in self.scalers.items():
                scaler_path = os.path.join(self.cache_dir, f"{name}_scaler.pkl")
                with open(scaler_path, 'wb') as f:
                    pickle.dump(scaler, f)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–ª–∞–≥ –æ–±—É—á–µ–Ω–∏—è
            with open(os.path.join(self.cache_dir, 'trained.flag'), 'w') as f:
                f.write('trained')
            
            print("‚úÖ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
    
    def _load_saved_models(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
        try:
            import pickle
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ñ–ª–∞–≥ –æ–±—É—á–µ–Ω–∏—è
            flag_path = os.path.join(self.cache_dir, 'trained.flag')
            if not os.path.exists(flag_path):
                return
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
            for name in self.models.keys():
                model_path = os.path.join(self.cache_dir, f"{name}_model.pkl")
                if os.path.exists(model_path):
                    with open(model_path, 'rb') as f:
                        self.models[name] = pickle.load(f)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∫–µ–π–ª–µ—Ä—ã
            for name in self.scalers.keys():
                scaler_path = os.path.join(self.cache_dir, f"{name}_scaler.pkl")
                if os.path.exists(scaler_path):
                    with open(scaler_path, 'rb') as f:
                        self.scalers[name] = pickle.load(f)
            
            self.models_trained = True
            print("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            self.models_trained = False
    
    def load_historical_data(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        try:
            if not os.path.exists(self.cache_file):
                print("‚ùå –§–∞–π–ª –∫—ç—à–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return pd.DataFrame()
            
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            trades = cache_data.get('trades', [])
            if not trades:
                print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ —Å–¥–µ–ª–∫–∞—Ö")
                return pd.DataFrame()
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
            df = pd.DataFrame(trades)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
            df['hour'] = df['timestamp'].dt.hour
            df['day_of_week'] = df['timestamp'].dt.dayofweek
            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
            
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–¥–µ–ª–æ–∫")
            return df
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return pd.DataFrame()
    
    def extract_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è ML –º–æ–¥–µ–ª–µ–π"""
        try:
            features_df = pd.DataFrame()
            
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            if 'hour' in df.columns:
                features_df['hour'] = df['hour']
            if 'day_of_week' in df.columns:
                features_df['day_of_week'] = df['day_of_week']
            if 'is_weekend' in df.columns:
                features_df['is_weekend'] = df['is_weekend']
            
            # –¢–æ—Ä–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            features_df['price'] = df['price'].astype(float)
            features_df['size'] = df['size'].astype(float)
            features_df['volume'] = features_df['price'] * features_df['size']
            
            # –ü—Ä–∏–∑–Ω–∞–∫–∏ —Ä—ã–Ω–∫–∞
            features_df['market_id'] = df['conditionId'].astype(str)
            features_df['outcome'] = df['outcome'].astype(str)
            features_df['outcome_encoded'] = (df['outcome'] == 'Up').astype(int)
            
            # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ —Ä—ã–Ω–∫—É
            market_stats = df.groupby('conditionId').agg({
                'size': ['mean', 'std', 'count'],
                'price': ['mean', 'std'],
                'timestamp': ['min', 'max']
            }).fillna(0)
            
            market_stats.columns = ['_'.join(col).strip() for col in market_stats.columns]
            market_stats = market_stats.reset_index()
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            features_df = features_df.merge(
                market_stats, 
                left_on='market_id', 
                right_on='conditionId', 
                how='left'
            )
            
            # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–æ—à–µ–ª—å–∫–æ–≤
            wallet_stats = df.groupby('proxyWallet').agg({
                'size': ['mean', 'std', 'count'],
                'price': ['mean', 'std']
            }).fillna(0)
            
            wallet_stats.columns = ['wallet_' + '_'.join(col).strip() for col in wallet_stats.columns]
            wallet_stats = wallet_stats.reset_index()
            
            # –î–æ–±–∞–≤–ª—è–µ–º proxyWallet –≤ features_df –¥–ª—è merge
            features_df['proxyWallet'] = df['proxyWallet']
            
            features_df = features_df.merge(
                wallet_stats,
                left_on='proxyWallet',
                right_on='proxyWallet',
                how='left'
            )
            
            # –£–¥–∞–ª—è–µ–º –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            numeric_columns = features_df.select_dtypes(include=[np.number]).columns
            features_df = features_df[numeric_columns].fillna(0)
            
            print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(features_df.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            return features_df
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return pd.DataFrame()
    
    def create_target_variable(self, df: pd.DataFrame) -> pd.Series:
        """–°–æ–∑–¥–∞–µ—Ç —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—Ç–∞–∫–∏)"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ç–∞–∫–∞ = –º–Ω–æ–≥–æ —Å–¥–µ–ª–æ–∫ –æ–¥–Ω–æ–≥–æ –∫–æ—à–µ–ª—å–∫–∞ –∑–∞ –∫–æ—Ä–æ—Ç–∫–æ–µ –≤—Ä–µ–º—è
            target = pd.Series(0, index=df.index)
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–æ—à–µ–ª—å–∫—É –∏ –≤—Ä–µ–º–µ–Ω–∏
            df_sorted = df.sort_values(['proxyWallet', 'timestamp'])
            
            for wallet in df['proxyWallet'].unique():
                wallet_trades = df_sorted[df_sorted['proxyWallet'] == wallet]
                
                if len(wallet_trades) < 3:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
                time_diffs = wallet_trades['timestamp'].diff().dt.total_seconds()
                
                # –ï—Å–ª–∏ –º–Ω–æ–≥–æ —Å–¥–µ–ª–æ–∫ –∑–∞ –∫–æ—Ä–æ—Ç–∫–æ–µ –≤—Ä–µ–º—è - –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é
                rapid_trades = (time_diffs < 300).sum()  # 5 –º–∏–Ω—É—Ç
                if rapid_trades >= 3:
                    target[wallet_trades.index] = 1
            
            print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {target.sum()} –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞—Ç–∞–∫")
            return target
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π: {e}")
            return pd.Series(0, index=df.index)
    
    def train_models(self) -> Dict[str, float]:
        """–û–±—É—á–∞–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
        try:
            print("üéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            df = self.load_historical_data()
            if df.empty:
                return {}
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = self.extract_features(df)
            if features.empty:
                return {}
            
            # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
            target = self.create_target_variable(df)
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test
            X_train, X_test, y_train, y_test = train_test_split(
                features, target, test_size=0.2, random_state=42, stratify=target
            )
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            if 'standard' in self.scalers:
                X_train_scaled = self.scalers['standard'].fit_transform(X_train)
                X_test_scaled = self.scalers['standard'].transform(X_test)
            else:
                X_train_scaled = X_train
                X_test_scaled = X_test
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
            model_scores = {}
            
            for name, model in self.models.items():
                try:
                    print(f"üîÑ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {name}")
                    
                    if name in ['logistic_regression'] and 'standard' in self.scalers:
                        model.fit(X_train_scaled, y_train)
                        y_pred = model.predict(X_test_scaled)
                    else:
                        model.fit(X_train, y_train)
                        y_pred = model.predict(X_test)
                    
                    score = accuracy_score(y_test, y_pred)
                    model_scores[name] = score
                    
                    print(f"‚úÖ {name}: —Ç–æ—á–Ω–æ—Å—Ç—å {score:.3f}")
                    
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è {name}: {e}")
                    model_scores[name] = 0.0
            
            print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {max(model_scores, key=model_scores.get)}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
            self.models_trained = True
            self._save_models()
            
            return model_scores
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
            return {}
    
    def predict_coordinated_attacks(self, recent_trades: List[Dict]) -> Dict:
        """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞—Ç–∞–∫"""
        try:
            if not recent_trades or not self.models:
                return {'risk_level': 'unknown', 'probability': 0.0, 'details': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –º–æ–¥–µ–ª–µ–π'}
            
            if not self.models_trained:
                return {'risk_level': 'unknown', 'probability': 0.0, 'details': '–ú–æ–¥–µ–ª–∏ –Ω–µ –æ–±—É—á–µ–Ω—ã. –í—ã–ø–æ–ª–Ω–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ.'}
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
            df = pd.DataFrame(recent_trades)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º timestamp
            if 'timestamp' in df.columns:
                if df['timestamp'].dtype == 'object':
                    # –ï—Å–ª–∏ timestamp —É–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ datetime
                    df['timestamp'] = pd.to_datetime(df['timestamp'])
                else:
                    # –ï—Å–ª–∏ timestamp –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
                    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                df['hour'] = df['timestamp'].dt.hour
                df['day_of_week'] = df['timestamp'].dt.dayofweek
                df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = self.extract_features(df)
            if features.empty:
                return {'risk_level': 'unknown', 'probability': 0.0, 'details': '–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø—Ä–∏–∑–Ω–∞–∫–∏'}
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
            predictions = {}
            probabilities = {}
            
            for name, model in self.models.items():
                try:
                    if name in ['logistic_regression'] and 'standard' in self.scalers:
                        features_scaled = self.scalers['standard'].transform(features)
                        pred = model.predict(features_scaled)
                        prob = model.predict_proba(features_scaled)[:, 1]
                    else:
                        pred = model.predict(features)
                        prob = model.predict_proba(features)[:, 1]
                    
                    predictions[name] = pred.mean()
                    probabilities[name] = prob.mean()
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è {name}: {e}")
                    predictions[name] = 0.0
                    probabilities[name] = 0.0
            
            # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            avg_probability = float(np.mean(list(probabilities.values())))
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞
            if avg_probability >= self.risk_thresholds['critical']:
                risk_level = 'critical'
            elif avg_probability >= self.risk_thresholds['high']:
                risk_level = 'high'
            elif avg_probability >= self.risk_thresholds['medium']:
                risk_level = 'medium'
            elif avg_probability >= self.risk_thresholds['low']:
                risk_level = 'low'
            else:
                risk_level = 'minimal'
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ Python —Ç–∏–ø—ã
            predictions_clean = {k: float(v) for k, v in predictions.items()}
            probabilities_clean = {k: float(v) for k, v in probabilities.items()}
            
            return {
                'risk_level': risk_level,
                'probability': avg_probability,
                'model_predictions': predictions_clean,
                'model_probabilities': probabilities_clean,
                'details': f'–ê–Ω–∞–ª–∏–∑ {len(recent_trades)} —Å–¥–µ–ª–æ–∫, {len(self.models)} –º–æ–¥–µ–ª–µ–π'
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return {'risk_level': 'error', 'probability': 0.0, 'details': str(e)}
    
    def analyze_trends(self, days_back: int = 7) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–Ω–¥—ã –∏ —Å–µ–∑–æ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        try:
            print(f"üìà –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {days_back} –¥–Ω–µ–π...")
            
            df = self.load_historical_data()
            if df.empty:
                return {}
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            cutoff_date = datetime.now() - timedelta(days=days_back)
            df_recent = df[df['timestamp'] >= cutoff_date]
            
            if df_recent.empty:
                return {}
            
            trends = {}
            
            # –ê–Ω–∞–ª–∏–∑ –ø–æ —á–∞—Å–∞–º
            hourly_activity = df_recent.groupby(df_recent['timestamp'].dt.hour).size()
            trends['peak_hours'] = hourly_activity.nlargest(3).to_dict()
            trends['quiet_hours'] = hourly_activity.nsmallest(3).to_dict()
            
            # –ê–Ω–∞–ª–∏–∑ –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏
            daily_activity = df_recent.groupby(df_recent['timestamp'].dt.day_name()).size()
            trends['daily_patterns'] = daily_activity.to_dict()
            
            # –ê–Ω–∞–ª–∏–∑ –æ–±—ä–µ–º–æ–≤
            volume_trends = df_recent.groupby(df_recent['timestamp'].dt.date).agg({
                'size': ['sum', 'mean', 'count']
            })
            trends['volume_trends'] = {
                'total_volume': {str(k): v for k, v in volume_trends[('size', 'sum')].to_dict().items()},
                'avg_trade_size': {str(k): v for k, v in volume_trends[('size', 'mean')].to_dict().items()},
                'trade_count': {str(k): v for k, v in volume_trends[('size', 'count')].to_dict().items()}
            }
            
            # –ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–æ–≤
            market_activity = df_recent.groupby('conditionId').size().nlargest(5)
            trends['top_markets'] = market_activity.to_dict()
            
            print("‚úÖ –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω")
            return trends
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
            return {}
    
    def generate_early_warning(self, current_risk: Dict) -> Optional[Dict]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞–Ω–Ω–µ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        try:
            if current_risk['risk_level'] in ['high', 'critical']:
                warning = {
                    'timestamp': datetime.now().isoformat(),
                    'risk_level': current_risk['risk_level'],
                    'probability': current_risk['probability'],
                    'message': self._get_warning_message(current_risk),
                    'recommendations': self._get_recommendations(current_risk),
                    'severity': 'high' if current_risk['risk_level'] == 'critical' else 'medium'
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
                self.alert_history.append(warning)
                
                print(f"üö® –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {warning['risk_level']}")
                return warning
            
            return None
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è: {e}")
            return None
    
    def _get_warning_message(self, risk: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è"""
        if risk['risk_level'] == 'critical':
            return f"üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –†–ò–°–ö! –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞—Ç–∞–∫–∏ ({risk['probability']:.1%})"
        elif risk['risk_level'] == 'high':
            return f"‚ö†Ô∏è –í–´–°–û–ö–ò–ô –†–ò–°–ö! –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ ({risk['probability']:.1%})"
        else:
            return f"‚ÑπÔ∏è –ü–æ–≤—ã—à–µ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å ({risk['probability']:.1%})"
    
    def _get_recommendations(self, risk: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–Ω–∏–∂–µ–Ω–∏—é —Ä–∏—Å–∫–∞"""
        recommendations = []
        
        if risk['risk_level'] == 'critical':
            recommendations.extend([
                "–ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ—à–µ–ª—å–∫–∏",
                "–£—Å–∏–ª–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ä—ã–Ω–∫–æ–≤",
                "–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ç–æ—Ä–≥–æ–≤",
                "–£–≤–µ–¥–æ–º–∏—Ç—å –∫–æ–º–∞–Ω–¥—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"
            ])
        elif risk['risk_level'] == 'high':
            recommendations.extend([
                "–£—Å–∏–ª–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π",
                "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–æ—Ä–≥–æ–≤",
                "–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π –Ω–∞ —Å–ª—É—á–∞–π —ç—Å–∫–∞–ª–∞—Ü–∏–∏"
            ])
        else:
            recommendations.extend([
                "–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥",
                "–û—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–∏—Ç—É–∞—Ü–∏–∏"
            ])
        
        return recommendations
    
    def get_risk_score(self, recent_trades: List[Dict]) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∏—Å–∫-—Å–∫–æ—Ä –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""
        try:
            if not recent_trades:
                return {'score': 0.0, 'level': 'minimal', 'factors': []}
            
            factors = []
            total_score = 0.0
            
            # –§–∞–∫—Ç–æ—Ä 1: –û–±—ä–µ–º —Ç–æ—Ä–≥–æ–≤
            total_volume = sum(trade.get('size', 0) for trade in recent_trades)
            volume_score = min(1.0, total_volume / 100000)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            factors.append({'name': '–û–±—ä–µ–º —Ç–æ—Ä–≥–æ–≤', 'score': volume_score, 'weight': 0.3})
            total_score += volume_score * 0.3
            
            # –§–∞–∫—Ç–æ—Ä 2: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ—à–µ–ª—å–∫–æ–≤
            unique_wallets = len(set(trade.get('proxyWallet', '') for trade in recent_trades))
            wallet_score = min(1.0, unique_wallets / 10)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            factors.append({'name': '–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–æ—à–µ–ª—å–∫–æ–≤', 'score': wallet_score, 'weight': 0.2})
            total_score += wallet_score * 0.2
            
            # –§–∞–∫—Ç–æ—Ä 3: –í—Ä–µ–º–µ–Ω–Ω–∞—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è
            if len(recent_trades) > 1:
                timestamps = [trade.get('timestamp', 0) for trade in recent_trades]
                if all(isinstance(ts, (int, float)) for ts in timestamps):
                    time_span = max(timestamps) - min(timestamps)
                    concentration_score = 1.0 - min(1.0, time_span / 3600)  # –ß–µ–º –º–µ–Ω—å—à–µ –≤—Ä–µ–º–µ–Ω–∏, —Ç–µ–º –≤—ã—à–µ —Ä–∏—Å–∫
                    factors.append({'name': '–í—Ä–µ–º–µ–Ω–Ω–∞—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è', 'score': concentration_score, 'weight': 0.3})
                    total_score += concentration_score * 0.3
                else:
                    # –ï—Å–ª–∏ timestamps –Ω–µ —á–∏—Å–ª–æ–≤—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
                    factors.append({'name': '–í—Ä–µ–º–µ–Ω–Ω–∞—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è', 'score': 0.5, 'weight': 0.3})
                    total_score += 0.5 * 0.3
            
            # –§–∞–∫—Ç–æ—Ä 4: –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ä—ã–Ω–∫–æ–≤
            unique_markets = len(set(trade.get('conditionId', '') for trade in recent_trades))
            market_diversity_score = min(1.0, unique_markets / 5)
            factors.append({'name': '–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ä—ã–Ω–∫–æ–≤', 'score': market_diversity_score, 'weight': 0.2})
            total_score += market_diversity_score * 0.2
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞
            if total_score >= 0.8:
                level = 'critical'
            elif total_score >= 0.6:
                level = 'high'
            elif total_score >= 0.4:
                level = 'medium'
            elif total_score >= 0.2:
                level = 'low'
            else:
                level = 'minimal'
            
            return {
                'score': round(total_score, 3),
                'level': level,
                'factors': factors,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ä–∏—Å–∫-—Å–∫–æ—Ä–∞: {e}")
            return {'score': 0.0, 'level': 'error', 'factors': [], 'error': str(e)}
    
    def get_analytics_summary(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–µ"""
        return {
            'models_available': len(self.models),
            'model_names': list(self.models.keys()),
            'models_trained': self.models_trained,
            'recent_alerts': len(self.alert_history),
            'feature_history_size': len(self.feature_history),
            'risk_thresholds': self.risk_thresholds,
            'last_updated': datetime.now().isoformat()
        }

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    analytics = PredictiveAnalytics()
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
    scores = analytics.train_models()
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è: {scores}")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–¥—ã
    trends = analytics.analyze_trends()
    print(f"–¢—Ä–µ–Ω–¥—ã: {trends}")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–≤–æ–¥–∫—É
    summary = analytics.get_analytics_summary()
    print(f"–°–≤–æ–¥–∫–∞: {summary}")
