import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from sklearn.ensemble import GradientBoostingClassifier, RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import joblib
import os
import json
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ ML –º–æ–¥–µ–ª–∏
from .advanced_ml_models import AdvancedMLModels

class PredictiveAnalyzer:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–æ–¥—É–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –¥–ª—è ShadowFlow
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç ML –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞—Ç–∞–∫
    """
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = data_dir
        self.models_dir = os.path.join(data_dir, "models")
        self.predictions_dir = os.path.join(data_dir, "predictions")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        os.makedirs(self.models_dir, exist_ok=True)
        os.makedirs(self.predictions_dir, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏
        self.attack_classifier = None
        self.risk_regressor = None
        self.trend_analyzer = None
        self.scaler = StandardScaler()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ ML –º–æ–¥–µ–ª–∏
        self.advanced_ml = AdvancedMLModels(data_dir)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        self.load_models()
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –∞–ª–µ—Ä—Ç–æ–≤
        self.risk_thresholds = {
            'low': 0.3,
            'medium': 0.6,
            'high': 0.8,
            'critical': 0.9
        }
    
    def load_models(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
        try:
            if os.path.exists(os.path.join(self.models_dir, "attack_classifier.pkl")):
                self.attack_classifier = joblib.load(os.path.join(self.models_dir, "attack_classifier.pkl"))
                print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∞—Ç–∞–∫")
            
            if os.path.exists(os.path.join(self.models_dir, "risk_regressor.pkl")):
                self.risk_regressor = joblib.load(os.path.join(self.models_dir, "risk_regressor.pkl"))
                print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä —Ä–∏—Å–∫–æ–≤")
            
            if os.path.exists(os.path.join(self.models_dir, "scaler.pkl")):
                self.scaler = joblib.load(os.path.join(self.models_dir, "scaler.pkl"))
                print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω —Å–∫–µ–π–ª–µ—Ä")
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
    
    def save_models(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
        try:
            if self.attack_classifier:
                joblib.dump(self.attack_classifier, os.path.join(self.models_dir, "attack_classifier.pkl"))
            
            if self.risk_regressor:
                joblib.dump(self.risk_regressor, os.path.join(self.models_dir, "risk_regressor.pkl"))
            
            joblib.dump(self.scaler, os.path.join(self.models_dir, "scaler.pkl"))
            print("‚úÖ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
    
    def extract_temporal_features(self, trades_data: List[Dict]) -> pd.DataFrame:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö –æ —Å–¥–µ–ª–∫–∞—Ö"""
        if not trades_data:
            return pd.DataFrame()
        
        df = pd.DataFrame(trades_data)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df = df.sort_values('timestamp')
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        df['day_of_month'] = df['timestamp'].dt.day
        df['month'] = df['timestamp'].dt.month
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –æ–∫–Ω–∞–º
        features = []
        
        # –ü–æ —á–∞—Å–∞–º
        hourly = df.groupby('hour').agg({
            'amount': ['count', 'sum', 'mean', 'std'],
            'price': ['mean', 'std', 'min', 'max']
        }).fillna(0)
        hourly.columns = [f'hourly_{col[0]}_{col[1]}' for col in hourly.columns]
        features.append(hourly)
        
        # –ü–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏
        daily = df.groupby('day_of_week').agg({
            'amount': ['count', 'sum', 'mean', 'std'],
            'price': ['mean', 'std', 'min', 'max']
        }).fillna(0)
        daily.columns = [f'daily_{col[0]}_{col[1]}' for col in daily.columns]
        features.append(daily)
        
        # –ü–æ —Ä—ã–Ω–∫–∞–º
        market = df.groupby('market_id').agg({
            'amount': ['count', 'sum', 'mean', 'std'],
            'price': ['mean', 'std', 'min', 'max']
        }).fillna(0)
        market.columns = [f'market_{col[0]}_{col[1]}' for col in market.columns]
        features.append(market)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if features:
            result = pd.concat(features, axis=1).fillna(0)
            return result
        else:
            return pd.DataFrame()
    
    def extract_coordination_features(self, clusters_data: List[Dict]) -> pd.DataFrame:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        if not clusters_data:
            return pd.DataFrame()
        
        df = pd.DataFrame(clusters_data)
        
        features = []
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        cluster_features = df.groupby('cluster_id').agg({
            'wallet_count': ['mean', 'std', 'max'],
            'total_volume': ['mean', 'std', 'max'],
            'sync_score': ['mean', 'std', 'max'],
            'time_window': ['mean', 'std', 'max']
        }).fillna(0)
        cluster_features.columns = [f'cluster_{col[0]}_{col[1]}' for col in cluster_features.columns]
        features.append(cluster_features)
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∫–æ—à–µ–ª—å–∫–æ–≤
        wallet_features = df.groupby('wallets').agg({
            'wallet_count': ['mean', 'std', 'max'],
            'total_volume': ['mean', 'std', 'max'],
            'sync_score': ['mean', 'std', 'max']
        }).fillna(0)
        wallet_features.columns = [f'wallet_{col[0]}_{col[1]}' for col in wallet_features.columns]
        features.append(wallet_features)
        
        if features:
            result = pd.concat(features, axis=1).fillna(0)
            return result
        else:
            return pd.DataFrame()
    
    def create_training_data(self, trades_data: List[Dict], clusters_data: List[Dict]) -> Tuple[np.ndarray, np.ndarray]:
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è ML –º–æ–¥–µ–ª–µ–π"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        temporal_features = self.extract_temporal_features(trades_data)
        coordination_features = self.extract_coordination_features(clusters_data)
        
        if temporal_features.empty and coordination_features.empty:
            return np.array([]), np.array([])
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        if not temporal_features.empty and not coordination_features.empty:
            # –ù–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ –∏–Ω–¥–µ–∫—Å—ã
            common_index = temporal_features.index.intersection(coordination_features.index)
            if len(common_index) > 0:
                X = pd.concat([
                    temporal_features.loc[common_index],
                    coordination_features.loc[common_index]
                ], axis=1).fillna(0)
            else:
                X = pd.concat([temporal_features, coordination_features], axis=1).fillna(0)
        elif not temporal_features.empty:
            X = temporal_features.fillna(0)
        else:
            X = coordination_features.fillna(0)
        
        # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: –µ—Å—Ç—å –ª–∏ –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ç–∞–∫–∞
        y_classification = np.zeros(len(X))
        if not clusters_data:
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã, –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∞—Ç–∞–∫–∏
            y_classification = np.ones(len(X))
        
        # –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏: —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞ (0-1)
        y_regression = np.random.uniform(0.1, 0.9, len(X))  # –ó–∞–≥–ª—É—à–∫–∞
        
        return X.values, y_classification, y_regression
    
    def train_models(self, trades_data: List[Dict], clusters_data: List[Dict]):
        """–û–±—É—á–∞–µ—Ç ML –º–æ–¥–µ–ª–∏ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
        
        # –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∞–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏
        print("ü§ñ –û–±—É—á–∞–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ ML –º–æ–¥–µ–ª–∏...")
        advanced_results = self.advanced_ml.train_advanced_models(trades_data, clusters_data)
        
        if advanced_results:
            print("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö
            best_classifier = None
            best_regressor = None
            
            for name, result in advanced_results.items():
                if 'classification' in name and 'f1' in result:
                    if best_classifier is None or result['f1'] > best_classifier[1]:
                        best_classifier = (result['model'], result['f1'])
                elif 'regression' in name and 'r2' in result:
                    if best_regressor is None or result['r2'] > best_regressor[1]:
                        best_regressor = (result['model'], result['r2'])
            
            if best_classifier:
                self.attack_classifier = best_classifier[0]
                print(f"‚úÖ –õ—É—á—à–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä: F1={best_classifier[1]:.3f}")
            
            if best_regressor:
                self.risk_regressor = best_regressor[0]
                print(f"‚úÖ –õ—É—á—à–∏–π —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä: R¬≤={best_regressor[1]:.3f}")
        
        # –ï—Å–ª–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –æ–±—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–µ
        if not self.attack_classifier or not self.risk_regressor:
            print("üîÑ –û–±—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏...")
            X, y_class, y_reg = self.create_training_data(trades_data, clusters_data)
            
            if len(X) == 0:
                print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test
            X_train, X_test, y_class_train, y_class_test, y_reg_train, y_reg_test = train_test_split(
                X, y_class, y_reg, test_size=0.2, random_state=42
            )
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_test_scaled = self.scaler.transform(X_test)
            
            # –û–±—É—á–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∞—Ç–∞–∫
            if not self.attack_classifier:
                print("üìä –û–±—É—á–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∞—Ç–∞–∫...")
                self.attack_classifier = GradientBoostingClassifier(
                    n_estimators=100,
                    learning_rate=0.1,
                    max_depth=6,
                    random_state=42
                )
                self.attack_classifier.fit(X_train_scaled, y_class_train)
            
            # –û–±—É—á–∞–µ–º —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä —Ä–∏—Å–∫–æ–≤
            if not self.risk_regressor:
                print("üìà –û–±—É—á–∞–µ–º —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä —Ä–∏—Å–∫–æ–≤...")
                self.risk_regressor = RandomForestRegressor(
                    n_estimators=100,
                    max_depth=10,
                    random_state=42
                )
                self.risk_regressor.fit(X_train_scaled, y_reg_train)
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            y_class_pred = self.attack_classifier.predict(X_test_scaled)
            y_reg_pred = self.risk_regressor.predict(X_test_scaled)
            
            accuracy = accuracy_score(y_class_test, y_class_pred)
            precision = precision_score(y_class_test, y_class_pred, zero_division=0)
            recall = recall_score(y_class_test, y_class_pred, zero_division=0)
            f1 = f1_score(y_class_test, y_class_pred, zero_division=0)
            
            mse = np.mean((y_reg_test - y_reg_pred) ** 2)
            mae = np.mean(np.abs(y_reg_test - y_reg_pred))
            
            print(f"‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä: Accuracy={accuracy:.3f}, Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}")
            print(f"‚úÖ –†–µ–≥—Ä–µ—Å—Å–æ—Ä: MSE={mse:.3f}, MAE={mae:.3f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        self.save_models()
    
    def predict_attack_probability(self, trades_data: List[Dict], clusters_data: List[Dict]) -> float:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞—Ç–∞–∫–∏"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏
        try:
            advanced_predictions = self.advanced_ml.predict_with_ensemble(trades_data, clusters_data)
            if advanced_predictions and 'attack_probability' in advanced_predictions:
                return advanced_predictions['attack_probability']
        except:
            pass
        
        # –ï—Å–ª–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ
        if not self.attack_classifier:
            return 0.5  # –ó–∞–≥–ª—É—à–∫–∞ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞
        
        X, _, _ = self.create_training_data(trades_data, clusters_data)
        if len(X) == 0:
            return 0.5
        
        X_scaled = self.scaler.transform(X)
        if hasattr(self.attack_classifier, 'predict_proba'):
            probability = self.attack_classifier.predict_proba(X_scaled)[0][1]
        else:
            probability = self.attack_classifier.predict(X_scaled)[0]
        return float(probability)
    
    def predict_risk_level(self, trades_data: List[Dict], clusters_data: List[Dict]) -> float:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞ (0-1)"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏
        try:
            advanced_predictions = self.advanced_ml.predict_with_ensemble(trades_data, clusters_data)
            if advanced_predictions and 'risk_level' in advanced_predictions:
                return advanced_predictions['risk_level']
        except:
            pass
        
        # –ï—Å–ª–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ
        if not self.risk_regressor:
            return 0.5  # –ó–∞–≥–ª—É—à–∫–∞ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞
        
        X, _, _ = self.create_training_data(trades_data, clusters_data)
        if len(X) == 0:
            return 0.5
        
        X_scaled = self.scaler.transform(X)
        risk = self.risk_regressor.predict(X_scaled)[0]
        return float(np.clip(risk, 0, 1))
    
    def analyze_trends(self, trades_data: List[Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–Ω–¥—ã –≤ —Ç–æ—Ä–≥–æ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        if not trades_data:
            return {}
        
        df = pd.DataFrame(trades_data)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df = df.sort_values('timestamp')
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        hourly_volume = df.groupby(df['timestamp'].dt.hour)['amount'].sum()
        daily_volume = df.groupby(df['timestamp'].dt.date)['amount'].sum()
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ —Ä—ã–Ω–∫–∞–º
        market_volume = df.groupby('market_id')['amount'].sum().sort_values(ascending=False)
        
        # –ê–Ω–∞–ª–∏–∑ —Ü–µ–Ω
        price_stats = df.groupby('market_id')['price'].agg(['mean', 'std', 'min', 'max'])
        
        trends = {
            'hourly_patterns': {
                'peak_hours': hourly_volume.nlargest(3).index.tolist(),
                'low_hours': hourly_volume.nsmallest(3).index.tolist(),
                'total_volume_by_hour': hourly_volume.to_dict()
            },
            'daily_patterns': {
                'volume_trend': daily_volume.tolist(),
                'dates': [str(d) for d in daily_volume.index]
            },
            'market_analysis': {
                'top_markets': market_volume.head(10).to_dict(),
                'price_volatility': price_stats['std'].to_dict()
            },
            'summary': {
                'total_trades': len(df),
                'total_volume': df['amount'].sum(),
                'avg_price': df['price'].mean(),
                'price_std': df['price'].std(),
                'unique_markets': df['market_id'].nunique(),
                'unique_wallets': df['wallet'].nunique()
            }
        }
        
        return trends
    
    def generate_early_warnings(self, trades_data: List[Dict], clusters_data: List[Dict]) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞–Ω–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∞—Ç–∞–∫–∞—Ö"""
        warnings = []
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∞—Ç–∞–∫–∏
        attack_prob = self.predict_attack_probability(trades_data, clusters_data)
        risk_level = self.predict_risk_level(trades_data, clusters_data)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–¥—ã
        trends = self.analyze_trends(trades_data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥–∏
        if attack_prob > self.risk_thresholds['high']:
            warnings.append({
                'type': 'attack_probability',
                'level': 'high',
                'message': f'–í—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞—Ç–∞–∫–∏: {attack_prob:.1%}',
                'timestamp': datetime.now().isoformat(),
                'details': {
                    'probability': attack_prob,
                    'risk_level': risk_level
                }
            })
        
        if risk_level > self.risk_thresholds['critical']:
            warnings.append({
                'type': 'risk_level',
                'level': 'critical',
                'message': f'–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞: {risk_level:.1%}',
                'timestamp': datetime.now().isoformat(),
                'details': {
                    'risk_level': risk_level,
                    'attack_probability': attack_prob
                }
            })
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if trends and 'hourly_patterns' in trends:
            peak_hours = trends['hourly_patterns']['peak_hours']
            if len(peak_hours) > 0:
                current_hour = datetime.now().hour
                if current_hour in peak_hours:
                    warnings.append({
                        'type': 'temporal_pattern',
                        'level': 'medium',
                        'message': f'–¢–µ–∫—É—â–∏–π —á–∞—Å ({current_hour}:00) —è–≤–ª—è–µ—Ç—Å—è –ø–∏–∫–æ–≤—ã–º –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏',
                        'timestamp': datetime.now().isoformat(),
                        'details': {
                            'current_hour': current_hour,
                            'peak_hours': peak_hours
                        }
                    })
        
        return warnings
    
    def get_predictions_summary(self, trades_data: List[Dict], clusters_data: List[Dict]) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
        attack_prob = self.predict_attack_probability(trades_data, clusters_data)
        risk_level = self.predict_risk_level(trades_data, clusters_data)
        trends = self.analyze_trends(trades_data)
        warnings = self.generate_early_warnings(trades_data, clusters_data)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞
        if risk_level >= self.risk_thresholds['critical']:
            risk_category = 'critical'
        elif risk_level >= self.risk_thresholds['high']:
            risk_category = 'high'
        elif risk_level >= self.risk_thresholds['medium']:
            risk_category = 'medium'
        else:
            risk_category = 'low'
        
        return {
            'attack_probability': attack_prob,
            'risk_level': risk_level,
            'risk_category': risk_category,
            'warnings_count': len(warnings),
            'warnings': warnings,
            'trends': trends,
            'timestamp': datetime.now().isoformat(),
            'model_status': {
                'classifier_loaded': self.attack_classifier is not None,
                'regressor_loaded': self.risk_regressor is not None,
                'scaler_loaded': self.scaler is not None
            }
        }
    
    def save_predictions(self, predictions: Dict, filename: str = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ —Ñ–∞–π–ª"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"predictions_{timestamp}.json"
        
        filepath = os.path.join(self.predictions_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(predictions, f, ensure_ascii=False, indent=2)
            print(f"‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filepath}")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {e}")
    
    def load_latest_predictions(self) -> Optional[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
        try:
            prediction_files = [f for f in os.listdir(self.predictions_dir) if f.startswith('predictions_')]
            if not prediction_files:
                return None
            
            latest_file = max(prediction_files)
            filepath = os.path.join(self.predictions_dir, latest_file)
            
            with open(filepath, 'r', encoding='utf-8') as f:
                return json.load(f)
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {e}")
            return None
