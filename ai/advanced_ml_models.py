import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import joblib
import os
import json
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

# ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ðµ ML Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸
from sklearn.ensemble import (
    GradientBoostingClassifier, RandomForestRegressor, 
    VotingClassifier, StackingClassifier, AdaBoostClassifier
)
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import (
    train_test_split, cross_val_score, GridSearchCV, 
    StratifiedKFold, TimeSeriesSplit
)
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, mean_squared_error, mean_absolute_error,
    classification_report, confusion_matrix
)
from sklearn.pipeline import Pipeline
# from imblearn.over_sampling import SMOTE
# from imblearn.under_sampling import RandomUnderSampler
# from imblearn.pipeline import Pipeline as ImbPipeline

# ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ðµ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð½Ñ‹Ðµ Ð±ÑƒÑÑ‚Ð¸Ð½Ð³Ð¸
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
except Exception:
    LIGHTGBM_AVAILABLE = False

try:
    from catboost import CatBoostClassifier, CatBoostRegressor
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
except Exception:
    CATBOOST_AVAILABLE = False

try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False

class AdvancedMLModels:
    """
    ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ð¹ Ð¼Ð¾Ð´ÑƒÐ»ÑŒ ML Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ ShadowFlow
    Ð’ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ ensemble Ð¼ÐµÑ‚Ð¾Ð´Ñ‹, Ð³Ð¸Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€ Ñ‚ÑŽÐ½Ð¸Ð½Ð³ Ð¸ Ð¿Ñ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ðµ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ñ‹
    """
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = data_dir
        self.models_dir = os.path.join(data_dir, "advanced_models")
        self.results_dir = os.path.join(data_dir, "ml_results")
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸
        os.makedirs(self.models_dir, exist_ok=True)
        os.makedirs(self.results_dir, exist_ok=True)
        
        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        self.models = {}
        self.scalers = {}
        self.encoders = {}
        self.feature_importance = {}
        self.model_performance = {}
        
        # Ð¤Ð»Ð°Ð³Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚Ð¸ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐº
        self.libs_available = {
            'xgboost': XGBOOST_AVAILABLE,
            'lightgbm': LIGHTGBM_AVAILABLE,
            'catboost': CATBOOST_AVAILABLE,
            'optuna': OPTUNA_AVAILABLE
        }
        
        print(f"ðŸ“š Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ ML Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸: {[k for k, v in self.libs_available.items() if v]}")
    
    def create_advanced_features(self, trades_data: List[Dict], clusters_data: List[Dict]) -> pd.DataFrame:
        """Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ Ð¿Ñ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð´Ð»Ñ ML Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹"""
        if not trades_data:
            return pd.DataFrame()
        
        df = pd.DataFrame(trades_data)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df = df.sort_values('timestamp')
        
        # Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸
        features = []
        
        # 1. Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸
        df['hour'] = df['timestamp'].dt.hour.astype(float)
        df['day_of_week'] = df['timestamp'].dt.dayofweek.astype(float)
        df['day_of_month'] = df['timestamp'].dt.day.astype(float)
        df['month'] = df['timestamp'].dt.month.astype(float)
        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int).astype(float)
        df['is_business_hours'] = df['hour'].between(9, 17).astype(int).astype(float)
        
        # 2. Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð¿Ð¾ ÐºÐ¾ÑˆÐµÐ»ÑŒÐºÐ°Ð¼
        wallet_stats = df.groupby('wallet').agg({
            'amount': ['count', 'sum', 'mean', 'std', 'min', 'max'],
            'price': ['mean', 'std', 'min', 'max'],
            'timestamp': ['min', 'max']
        }).fillna(0)
        wallet_stats.columns = [f'wallet_{col[0]}_{col[1]}' for col in wallet_stats.columns]
        
        # 3. Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð¿Ð¾ Ñ€Ñ‹Ð½ÐºÐ°Ð¼
        market_stats = df.groupby('market_id').agg({
            'amount': ['count', 'sum', 'mean', 'std', 'min', 'max'],
            'price': ['mean', 'std', 'min', 'max'],
            'wallet': 'nunique'
        }).fillna(0)
        market_stats.columns = [f'market_{col[0]}_{col[1]}' for col in market_stats.columns]
        
        # 4. ÐŸÑ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ
        df['amount_price_interaction'] = df['amount'] * df['price']
        df['hour_day_interaction'] = df['hour'] * df['day_of_week']
        df['volume_intensity'] = df['amount'] / (df.groupby('market_id')['amount'].transform('mean') + 1e-6)
        
        # 5. ÐŸÑ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð°Ð½Ð¾Ð¼Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸
        for col in ['amount', 'price']:
            df[f'{col}_zscore'] = (df[col] - df[col].mean()) / (df[col].std() + 1e-6)
            df[f'{col}_is_outlier'] = (abs(df[f'{col}_zscore']) > 3).astype(int).astype(float)
        
        # 6. Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¾ÐºÐ½Ð° (ÑÐºÐ¾Ð»ÑŒÐ·ÑÑ‰Ð¸Ðµ ÑÑ€ÐµÐ´Ð½Ð¸Ðµ)
        df_sorted = df.sort_values('timestamp')
        for window in [5, 10, 30, 60]:  # Ð¼Ð¸Ð½ÑƒÑ‚Ñ‹
            df_sorted[f'amount_ma_{window}'] = df_sorted['amount'].rolling(window=window, min_periods=1).mean()
            df_sorted[f'price_ma_{window}'] = df_sorted['price'].rolling(window=window, min_periods=1).mean()
            df_sorted[f'volume_ma_{window}'] = df_sorted['amount'].rolling(window=window, min_periods=1).sum()
        
        # 7. ÐŸÑ€Ð¸Ð·Ð½Ð°ÐºÐ¸ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¾ÑÑ‚Ð¸ (ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ñ‹)
        if clusters_data:
            cluster_df = pd.DataFrame(clusters_data)
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ Ð½ÑƒÐ¶Ð½Ñ‹Ñ… ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº
            if 'cluster_id' in cluster_df.columns:
                cluster_features = cluster_df.groupby('cluster_id').agg({
                    'wallet_count': ['mean', 'std', 'max'],
                    'total_volume': ['mean', 'std', 'max'],
                    'sync_score': ['mean', 'std', 'max'],
                    'time_window': ['mean', 'std', 'max']
                }).fillna(0)
                cluster_features.columns = [f'cluster_{col[0]}_{col[1]}' for col in cluster_features.columns]
        
        # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð²ÑÐµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸
        feature_cols = [
            'hour', 'day_of_week', 'day_of_month', 'month', 'is_weekend', 'is_business_hours',
            'amount', 'price', 'amount_price_interaction', 'hour_day_interaction', 'volume_intensity',
            'amount_zscore', 'price_zscore', 'amount_is_outlier', 'price_is_outlier'
        ]
        
        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÑÐºÐ¾Ð»ÑŒÐ·ÑÑ‰Ð¸Ðµ ÑÑ€ÐµÐ´Ð½Ð¸Ðµ
        ma_cols = [col for col in df_sorted.columns if col.startswith(('amount_ma_', 'price_ma_', 'volume_ma_'))]
        feature_cols.extend(ma_cols)
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð´Ð°Ñ‚Ð°Ñ„Ñ€ÐµÐ¹Ð¼, Ð¸ÑÐºÐ»ÑŽÑ‡Ð°Ñ Ð½ÐµÑ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸
        available_cols = [col for col in feature_cols if col in df_sorted.columns]
        final_df = df_sorted[available_cols].fillna(0)
        
        # Ð˜ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ datetime Ð¸ object ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸
        numeric_cols = final_df.select_dtypes(include=[np.number]).columns
        final_df = final_df[numeric_cols]
        
        # Ð£Ð±ÐµÐ¶Ð´Ð°ÐµÐ¼ÑÑ, Ñ‡Ñ‚Ð¾ Ð²ÑÐµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ðµ
        for col in final_df.columns:
            if final_df[col].dtype == 'bool':
                final_df[col] = final_df[col].astype(float)
        
        # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð²ÑÐµ Ð² float Ð´Ð»Ñ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ñ ML Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸
        final_df = final_df.astype(float)
        
        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð¿Ð¾ ÐºÐ¾ÑˆÐµÐ»ÑŒÐºÐ°Ð¼ Ð¸ Ñ€Ñ‹Ð½ÐºÐ°Ð¼
        if not wallet_stats.empty:
            wallet_features = df_sorted.merge(wallet_stats, left_on='wallet', right_index=True, how='left')
            wallet_cols = [col for col in wallet_features.columns if col.startswith('wallet_')]
            final_df = pd.concat([final_df, wallet_features[wallet_cols].fillna(0)], axis=1)
        
        if not market_stats.empty:
            market_features = df_sorted.merge(market_stats, left_on='market_id', right_index=True, how='left')
            market_cols = [col for col in market_features.columns if col.startswith('market_')]
            final_df = pd.concat([final_df, market_features[market_cols].fillna(0)], axis=1)
        
        return final_df.fillna(0)
    
    def create_target_variables(self, trades_data: List[Dict], clusters_data: List[Dict]) -> Tuple[np.ndarray, np.ndarray]:
        """Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ Ñ†ÐµÐ»ÐµÐ²Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
        if not trades_data:
            return np.array([]), np.array([])
        
        df = pd.DataFrame(trades_data)
        
        # Ð”Ð»Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸: ÐµÑÑ‚ÑŒ Ð»Ð¸ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð°Ñ‚Ð°ÐºÐ°
        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð² ÐºÐ°Ðº Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€ Ð°Ñ‚Ð°Ðº
        y_classification = np.zeros(len(df))
        
        if clusters_data:
            # Ð•ÑÐ»Ð¸ ÐµÑÑ‚ÑŒ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ñ‹, Ð¿Ð¾Ð¼ÐµÑ‡Ð°ÐµÐ¼ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ ÑÐ´ÐµÐ»ÐºÐ¸ ÐºÐ°Ðº Ð°Ñ‚Ð°ÐºÐ¸
            cluster_wallets = set()
            for cluster in clusters_data:
                if 'wallets' in cluster:
                    wallets = cluster['wallets'].split(',') if isinstance(cluster['wallets'], str) else cluster['wallets']
                    cluster_wallets.update(wallets)
            
            # ÐŸÐ¾Ð¼ÐµÑ‡Ð°ÐµÐ¼ ÑÐ´ÐµÐ»ÐºÐ¸ Ð¾Ñ‚ ÐºÐ¾ÑˆÐµÐ»ÑŒÐºÐ¾Ð² Ð¸Ð· ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð² ÐºÐ°Ðº Ð°Ñ‚Ð°ÐºÐ¸
            y_classification = df['wallet'].isin(cluster_wallets).astype(int)
        
        # Ð”Ð»Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð¸: ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ Ñ€Ð¸ÑÐºÐ° (0-1)
        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð°Ñ†Ð¸ÑŽ Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ð¾Ð²
        risk_factors = []
        
        # Ð¤Ð°ÐºÑ‚Ð¾Ñ€ 1: Ð¾Ð±ÑŠÐµÐ¼ ÑÐ´ÐµÐ»ÐºÐ¸ Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÑÑ€ÐµÐ´Ð½ÐµÐ³Ð¾
        avg_amount = df['amount'].mean()
        volume_factor = np.clip(df['amount'] / (avg_amount + 1e-6), 0, 2)
        
        # Ð¤Ð°ÐºÑ‚Ð¾Ñ€ 2: Ð²Ñ€ÐµÐ¼Ñ (Ð½Ð¾Ñ‡Ð½Ñ‹Ðµ Ñ‡Ð°ÑÑ‹ Ð±Ð¾Ð»ÐµÐµ Ð¿Ð¾Ð´Ð¾Ð·Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        hour_factor = np.where(df['timestamp'].dt.hour.between(0, 6), 1.5, 1.0)
        
        # Ð¤Ð°ÐºÑ‚Ð¾Ñ€ 3: Ð¿Ñ€Ð¸Ð½Ð°Ð´Ð»ÐµÐ¶Ð½Ð¾ÑÑ‚ÑŒ Ðº ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ñƒ
        cluster_factor = y_classification * 1.5 + (1 - y_classification) * 0.5
        
        # ÐšÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€ÑƒÐµÐ¼ Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ñ‹
        y_regression = np.clip(
            (volume_factor * hour_factor * cluster_factor) / 3.0, 
            0.1, 1.0
        )
        
        return y_classification, y_regression
    
    def create_ensemble_models(self) -> Dict[str, Any]:
        """Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ ensemble Ð¼Ð¾Ð´ÐµÐ»Ð¸"""
        models = {}
        
        # 1. Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        models['logistic'] = LogisticRegression(random_state=42, max_iter=1000)
        models['random_forest'] = RandomForestRegressor(n_estimators=100, random_state=42)
        models['gradient_boosting'] = GradientBoostingClassifier(n_estimators=100, random_state=42)
        models['svm'] = SVC(probability=True, random_state=42)
        models['neural_network'] = MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=500)
        
        # 2. ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹)
        if self.libs_available['xgboost']:
            try:
                models['xgboost'] = xgb.XGBClassifier(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    random_state=42,
                    eval_metric='logloss'
                )
                models['xgboost_reg'] = xgb.XGBRegressor(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    random_state=42
                )
            except Exception as e:
                print(f"âš ï¸ XGBoost Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½: {e}")
        
        if self.libs_available['lightgbm']:
            try:
                models['lightgbm'] = lgb.LGBMClassifier(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    random_state=42,
                    verbose=-1
                )
                models['lightgbm_reg'] = lgb.LGBMRegressor(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    random_state=42,
                    verbose=-1
                )
            except Exception as e:
                print(f"âš ï¸ LightGBM Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½: {e}")
        
        if self.libs_available['catboost']:
            try:
                models['catboost'] = CatBoostClassifier(
                    iterations=100,
                    depth=6,
                    learning_rate=0.1,
                    random_state=42,
                    verbose=False
                )
                models['catboost_reg'] = CatBoostRegressor(
                    iterations=100,
                    depth=6,
                    learning_rate=0.1,
                    random_state=42,
                    verbose=False
                )
            except Exception as e:
                print(f"âš ï¸ CatBoost Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½: {e}")
        
        # 3. Ensemble Ð¼Ð¾Ð´ÐµÐ»Ð¸
        # Voting Classifier
        voting_models = [
            ('logistic', models['logistic']),
            ('gradient_boosting', models['gradient_boosting']),
            ('neural_network', models['neural_network'])
        ]
        
        if self.libs_available['xgboost']:
            voting_models.append(('xgboost', models['xgboost']))
        
        models['voting_classifier'] = VotingClassifier(
            estimators=voting_models,
            voting='soft'
        )
        
        # Stacking Classifier
        models['stacking_classifier'] = StackingClassifier(
            estimators=voting_models,
            final_estimator=LogisticRegression(),
            cv=3
        )
        
        return models
    
    def train_advanced_models(self, trades_data: List[Dict], clusters_data: List[Dict]) -> Dict[str, Any]:
        """ÐžÐ±ÑƒÑ‡Ð°ÐµÑ‚ Ð¿Ñ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ðµ ML Ð¼Ð¾Ð´ÐµÐ»Ð¸"""
        print("ðŸš€ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ñ… ML Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹...")
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð¸ Ñ†ÐµÐ»ÐµÐ²Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ
        X = self.create_advanced_features(trades_data, clusters_data)
        y_class, y_reg = self.create_target_variables(trades_data, clusters_data)
        
        if len(X) == 0 or len(y_class) == 0:
            print("âš ï¸ ÐÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ")
            return {}
        
        print(f"ðŸ“Š Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ {X.shape[1]} Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² Ð¸Ð· {len(trades_data)} ÑÐ´ÐµÐ»Ð¾Ðº")
        
        # Ð Ð°Ð·Ð´ÐµÐ»ÑÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
        X_train, X_test, y_class_train, y_class_test, y_reg_train, y_reg_test = train_test_split(
            X, y_class, y_reg, test_size=0.2, random_state=42, stratify=y_class
        )
        
        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
        scaler = StandardScaler()
        
        # Ð£Ð±ÐµÐ¶Ð´Ð°ÐµÐ¼ÑÑ, Ñ‡Ñ‚Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ðµ
        # Ð˜ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ datetime ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸
        numeric_cols = X_train.select_dtypes(include=[np.number]).columns
        X_train = X_train[numeric_cols]
        X_test = X_test[numeric_cols]
        
        X_train = X_train.astype(float)
        X_test = X_test.astype(float)
        
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        self.scalers['main'] = scaler
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        models = self.create_ensemble_models()
        
        results = {}
        
        # ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ñ‹
        print("ðŸ“ˆ ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ñ‹...")
        classification_models = {}
        
        # Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        if 'logistic' in models:
            classification_models['logistic'] = models['logistic']
        if 'gradient_boosting' in models:
            classification_models['gradient_boosting'] = models['gradient_boosting']
        if 'neural_network' in models:
            classification_models['neural_network'] = models['neural_network']
        
        # ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        if 'xgboost' in models:
            classification_models['xgboost'] = models['xgboost']
        if 'lightgbm' in models:
            classification_models['lightgbm'] = models['lightgbm']
        if 'catboost' in models:
            classification_models['catboost'] = models['catboost']
        
        # Ensemble Ð¼Ð¾Ð´ÐµÐ»Ð¸
        if 'voting_classifier' in models:
            classification_models['voting_classifier'] = models['voting_classifier']
        if 'stacking_classifier' in models:
            classification_models['stacking_classifier'] = models['stacking_classifier']
        
        for name, model in classification_models.items():
            try:
                print(f"  ðŸ”„ ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ {name}...")
                model.fit(X_train_scaled, y_class_train)
                
                # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ
                y_pred = model.predict(X_test_scaled)
                y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else y_pred
                
                # ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸
                accuracy = accuracy_score(y_class_test, y_pred)
                precision = precision_score(y_class_test, y_pred, zero_division=0)
                recall = recall_score(y_class_test, y_pred, zero_division=0)
                f1 = f1_score(y_class_test, y_pred, zero_division=0)
                auc = roc_auc_score(y_class_test, y_pred_proba) if len(np.unique(y_class_test)) > 1 else 0
                
                results[f'{name}_classification'] = {
                    'model': model,
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1': f1,
                    'auc': auc,
                    'predictions': y_pred,
                    'probabilities': y_pred_proba
                }
                
                print(f"    âœ… {name}: Accuracy={accuracy:.3f}, F1={f1:.3f}, AUC={auc:.3f}")
                
            except Exception as e:
                print(f"    âŒ {name}: {str(e)}")
        
        # ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¾Ñ€Ñ‹
        print("ðŸ“Š ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¾Ñ€Ñ‹...")
        regression_models = {}
        
        # Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        if 'random_forest' in models:
            regression_models['random_forest'] = models['random_forest']
        
        # ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        if 'xgboost_reg' in models:
            regression_models['xgboost'] = models['xgboost_reg']
        if 'lightgbm_reg' in models:
            regression_models['lightgbm'] = models['lightgbm_reg']
        if 'catboost_reg' in models:
            regression_models['catboost'] = models['catboost_reg']
        
        # Ensemble Ð¼Ð¾Ð´ÐµÐ»Ð¸
        if 'voting_regressor' in models:
            regression_models['voting_regressor'] = models['voting_regressor']
        
        for name, model in regression_models.items():
            try:
                print(f"  ðŸ”„ ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ {name}...")
                model.fit(X_train_scaled, y_reg_train)
                
                # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ
                y_pred = model.predict(X_test_scaled)
                
                # ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸
                mse = mean_squared_error(y_reg_test, y_pred)
                mae = mean_absolute_error(y_reg_test, y_pred)
                r2 = model.score(X_test_scaled, y_reg_test)
                
                results[f'{name}_regression'] = {
                    'model': model,
                    'mse': mse,
                    'mae': mae,
                    'r2': r2,
                    'predictions': y_pred
                }
                
                print(f"    âœ… {name}: MSE={mse:.3f}, MAE={mae:.3f}, RÂ²={r2:.3f}")
                
            except Exception as e:
                print(f"    âŒ {name}: {str(e)}")
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
        self.model_performance = results
        self.save_models()
        
        print("ðŸŽ‰ ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾!")
        return results
    
    def predict_with_ensemble(self, trades_data: List[Dict], clusters_data: List[Dict]) -> Dict[str, float]:
        """Ð”ÐµÐ»Ð°ÐµÑ‚ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ ensemble Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹"""
        if not self.model_performance:
            return {'attack_probability': 0.5, 'risk_level': 0.5}
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸
        X = self.create_advanced_features(trades_data, clusters_data)
        if len(X) == 0:
            return {'attack_probability': 0.5, 'risk_level': 0.5}
        
        # Ð£Ð±ÐµÐ¶Ð´Ð°ÐµÐ¼ÑÑ, Ñ‡Ñ‚Ð¾ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ð¼
        # Ð˜ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ datetime Ð¸ object ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸
        numeric_cols = X.select_dtypes(include=[np.number]).columns
        X = X[numeric_cols]
        X = X.astype(float)
        
        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼
        if 'main' in self.scalers:
            X_scaled = self.scalers['main'].transform(X)
        else:
            X_scaled = X.values
        
        # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð¾Ð²
        attack_probs = []
        for name, result in self.model_performance.items():
            if 'classification' in name and 'model' in result:
                try:
                    if hasattr(result['model'], 'predict_proba'):
                        prob = result['model'].predict_proba(X_scaled)[:, 1].mean()
                    else:
                        prob = result['model'].predict(X_scaled).mean()
                    attack_probs.append(prob)
                except:
                    continue
        
        # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¾Ñ€Ð¾Ð²
        risk_levels = []
        for name, result in self.model_performance.items():
            if 'regression' in name and 'model' in result:
                try:
                    risk = result['model'].predict(X_scaled).mean()
                    risk_levels.append(risk)
                except:
                    continue
        
        # Ð£ÑÑ€ÐµÐ´Ð½ÑÐµÐ¼ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ
        attack_probability = np.mean(attack_probs) if attack_probs else 0.5
        risk_level = np.mean(risk_levels) if risk_levels else 0.5
        
        return {
            'attack_probability': float(np.clip(attack_probability, 0, 1)),
            'risk_level': float(np.clip(risk_level, 0, 1))
        }
    
    def save_models(self):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸"""
        try:
            for name, result in self.model_performance.items():
                if 'model' in result:
                    model_path = os.path.join(self.models_dir, f"{name}.pkl")
                    joblib.dump(result['model'], model_path)
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑÐºÐµÐ¹Ð»ÐµÑ€Ñ‹
            for name, scaler in self.scalers.items():
                scaler_path = os.path.join(self.models_dir, f"scaler_{name}.pkl")
                joblib.dump(scaler, scaler_path)
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
            metrics_path = os.path.join(self.results_dir, "model_performance.json")
            metrics_data = {}
            for name, result in self.model_performance.items():
                metrics_data[name] = {
                    k: v for k, v in result.items() 
                    if k != 'model' and k != 'predictions' and k != 'probabilities'
                }
            
            with open(metrics_path, 'w') as f:
                json.dump(metrics_data, f, indent=2)
            
            print("âœ… ÐœÐ¾Ð´ÐµÐ»Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹")
            
        except Exception as e:
            print(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹: {e}")
    
    def load_models(self):
        """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸"""
        try:
            # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
            metrics_path = os.path.join(self.results_dir, "model_performance.json")
            if os.path.exists(metrics_path):
                with open(metrics_path, 'r') as f:
                    metrics_data = json.load(f)
                
                # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸
                for name in metrics_data.keys():
                    model_path = os.path.join(self.models_dir, f"{name}.pkl")
                    if os.path.exists(model_path):
                        model = joblib.load(model_path)
                        self.model_performance[name] = {
                            'model': model,
                            **metrics_data[name]
                        }
                
                print(f"âœ… Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {len(self.model_performance)} Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹")
            
            # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÑÐºÐµÐ¹Ð»ÐµÑ€Ñ‹
            for scaler_file in os.listdir(self.models_dir):
                if scaler_file.startswith('scaler_'):
                    scaler_name = scaler_file.replace('scaler_', '').replace('.pkl', '')
                    scaler_path = os.path.join(self.models_dir, scaler_file)
                    self.scalers[scaler_name] = joblib.load(scaler_path)
                
        except Exception as e:
            print(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹: {e}")
    
    def get_model_summary(self) -> Dict[str, Any]:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÐ²Ð¾Ð´ÐºÑƒ Ð¿Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼"""
        if not self.model_performance:
            return {'models_loaded': 0, 'best_classifier': None, 'best_regressor': None}
        
        # ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ Ð»ÑƒÑ‡ÑˆÐ¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        best_classifier = None
        best_classifier_score = 0
        
        best_regressor = None
        best_regressor_score = float('inf')
        
        for name, result in self.model_performance.items():
            if 'classification' in name:
                if 'f1' in result and result['f1'] > best_classifier_score:
                    best_classifier = name
                    best_classifier_score = result['f1']
            elif 'regression' in name:
                if 'mse' in result and result['mse'] < best_regressor_score:
                    best_regressor = name
                    best_regressor_score = result['mse']
        
        return {
            'models_loaded': len(self.model_performance),
            'best_classifier': best_classifier,
            'best_regressor': best_regressor,
            'available_libraries': self.libs_available,
            'model_details': {
                name: {k: v for k, v in result.items() if k != 'model'}
                for name, result in self.model_performance.items()
            }
        }
